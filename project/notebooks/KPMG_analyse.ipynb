{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPMG TEST NOTEBOOK\n",
    "\n",
    "Selenium is a tool initially created to automate tests on websites. It is therefore very useful when information is accessible by clicking on links. A button for example is an element from which it is very difficult to obtain the link. BeautifulSoup then becomes limited.\n",
    "In this case, use Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_news_md\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"nl_core_news_md\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "def dict_sort(dict_uniqwords:dict ) -> dict:\n",
    "    from operator import itemgetter\n",
    "    srt=sorted(dict_uniqwords.items(), key=itemgetter(1),reverse=True)\n",
    "    return srt\n",
    "\n",
    "def getsomestats(dict_uniqwords:dict ,txt_original:str, topcnt:int = 15):\n",
    "    #---NOT USED FOR NOW\n",
    "\n",
    "    #print(\"@@@@@@@@@@@@@@@@@@@@\",dict_uniqwords)\n",
    "    # total number of unique words in doc\n",
    "    print(\"unique word count :\", len(dict_uniqwords))\n",
    "\n",
    "    # total number of words in doc\n",
    "    wordcnt=0\n",
    "    for s in dict_uniqwords: wordcnt+=dict_uniqwords[s]\n",
    "    print(\"total wordcount: \",wordcnt)\n",
    "    \n",
    "    #sorted words count\n",
    "    ###from operator import itemgetter\n",
    "    ###srt=sorted(dict_uniqwords.items(), key=itemgetter(1),reverse=True)\n",
    "    srt = dict_sort(dict_uniqwords)\n",
    "\n",
    "    res=[]\n",
    "    for i in range(topcnt):\n",
    "        try:\n",
    "            res.append(srt[i][0])\n",
    "            print(srt[i])\n",
    "        except:\n",
    "            break\n",
    "    #print original doc\n",
    "    #print(txt_original)\n",
    "    return res\n",
    "\n",
    "def nlp_cleanandlemmatize(txtdoc: str, list_wordstoskip:str = ''):\n",
    "    LANG='nl'\n",
    "    # Load the model\n",
    "    #nlp = spacy.load(\"nl_core_news_md\")\n",
    "    #stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "    # words to discard\n",
    "    months={'nl':['januari','februari','maart','april','mei','juni','augustus','september','oktober','november','december'],\n",
    "            'fr':['janvier','fevrier','mars','avril','mai','juin','juillet','aout','septembre','octobre','decembre']}\n",
    "    days={'nl':['maandag','dinsdag','woensdag','donderdag','vrijdag','zaterdag','zondag'],\n",
    "            'fr':['lundi','mardi','mercredi','jeudi','vendredi','samedi','dimanche']}\n",
    "    # \n",
    "    nlp.max_length=10000000\n",
    "    nlp_doc=nlp(txtdoc)\n",
    "    list_allwordslemmatized=[]\n",
    "    dict_uniqwords={}\n",
    "    list_tokens=[]\n",
    "    #sequential filter\n",
    "    for token in nlp_doc:\n",
    "        lemma_lower=token.lemma_.lower()\n",
    "        if token in stopwords:\n",
    "            continue\n",
    "        if (token.is_punct or token.is_space or token.is_stop):\n",
    "            continue\n",
    "        if token.text.isdecimal():\n",
    "            continue\n",
    "        if True in [char.isdigit() for char in token.text]:\n",
    "            continue\n",
    "        if token.text[-1] == '.':\n",
    "            continue\n",
    "        if len(token.text) <= 2:\n",
    "            continue\n",
    "        if lemma_lower in months[LANG]:\n",
    "            continue\n",
    "        if lemma_lower in days[LANG]:\n",
    "            continue\n",
    "        if lemma_lower in list_wordstoskip:\n",
    "            continue\n",
    "        #pass only nouns\n",
    "        #print(token.pos_,token.pos)\n",
    "        if token.pos_ != 'NOUN':\n",
    "            continue\n",
    "        #create dict of unique words with count\n",
    "        if lemma_lower not in dict_uniqwords: \n",
    "            dict_uniqwords[lemma_lower]=1\n",
    "            #save tokens for vector comparison\n",
    "            list_tokens.append(token)\n",
    "        else:\n",
    "            dict_uniqwords[lemma_lower]+=1\n",
    "        ##save tokens for vector comparison\n",
    "        ##list_tokens.append(token)\n",
    "\n",
    "        \n",
    "        #saving the words (lemma equates number   -   lemma_ equates word)\n",
    "        ###list_allwordslemmatized.append([token.lemma,lemma_lower,token.text])\n",
    "    return dict_uniqwords, list_tokens\n",
    "    #end of filter-----------------------------------------------\n",
    "\n",
    "def concat_docs(docs:list[str] ,nmbr:int = 0) -> str:\n",
    "    #concat nmbr dcocuments from doc return txtdoc\n",
    "    #nmbr=10 #test anz docus\n",
    "    txtdoc=''\n",
    "    #for txt in df['cleantextnl'].values:\n",
    "    for txt in docs:\n",
    "        txtdoc+=str(txt)\n",
    "        if nmbr == 1:\n",
    "            break\n",
    "        nmbr-=1\n",
    "    return txtdoc\n",
    "\n",
    "#read all docs\n",
    "df = pd.read_pickle(\"../data/Staatsblad_nl_fr.pkl\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare testdata dict_uniqwords, list_sorted_uniqwords, list_tokens\n",
    "\n",
    "numberofdocumenttoscan=10\n",
    "\n",
    "for n in range(numberofdocumenttoscan):\n",
    "    txtdoc = df['cleantextnl'].values[n]\n",
    "\n",
    "    list_wordstoskip=['artikel','document'] #add here words to discard\n",
    "\n",
    "    dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txtdoc,list_wordstoskip)\n",
    "\n",
    "    list_sorted_uniqwords = dict_sort(dict_uniqwords)\n",
    "\n",
    "    ###print(\"Remaining words after filtering: \",len(list_allwordslemmatized))\n",
    "    #topcnt=10 \n",
    "    #list_word_topcnt=getsomestats(dict_uniqwords,txtdoc,topcnt)\n",
    "    #print(\"Top\",topcnt,\": \",list_word_topcnt)\n",
    "    ##################################################################\n",
    "    print(\"---------------------------------------------------\")\n",
    "    ##################################################################\n",
    "    # Output:   list_sorted_uniqwords , dict_uniqwords, list_tokens\n",
    "    #\n",
    "    print(list_sorted_uniqwords[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare a token against a tokenslist\n",
    "def token_compare(token_totest,list_tokens,min_score:int = 0.6):\n",
    "    #return list of tokens with similarity >= min_score \n",
    "    list_tokens_toreturn=[]\n",
    "    tot_similar_word=0\n",
    "    istax=False\n",
    "    for token in list_tokens:\n",
    "        #print(\"token_compare\",token)\n",
    "        similar=token_totest.similarity(token)\n",
    "        #print(token_totest, \"<->\", token, similar)\n",
    "        if similar >= min_score:\n",
    "            list_tokens_toreturn.append([token_totest,token,similar])\n",
    "            print(\"Passed:\",token_totest,token,similar)\n",
    "            #positive for tax\n",
    "            istax=True\n",
    "            tot_similar_word+=similar\n",
    "    return istax, tot_similar_word, list_tokens_toreturn\n",
    "\n",
    "def createlistofkeywords(numberofdocumenttoscan:int = 50,similar_doc:int = 0,min_score:int = 0.6, list_testwords:list = ['belasting','tax','venootschapsbelasting']):\n",
    "    #numberofdocumenttoscan=50\n",
    "    list_keep_tax_words=[]\n",
    "    list_keep_pointer_taxdocs=[]\n",
    "    for n in range(numberofdocumenttoscan):\n",
    "        #print(\"$$$$$$$$$$\",n,numberofdocumenttoscan)\n",
    "        txtdoc = df['cleantextnl'].values[n]\n",
    "        list_wordstoskip=['artikel','document'] #add here words to discard\n",
    "        dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txtdoc,list_wordstoskip) #get tokens\n",
    "        #list_sorted_uniqwords = dict_sort(dict_uniqwords) #sort\n",
    "        \n",
    "        #print(n,\"--------------------------\")\n",
    "        #words to check for\n",
    "\n",
    "        #tokenize and check\n",
    "        tot_similar_doc=0\n",
    "        istax_doc=False\n",
    "        for word in list_testwords:\n",
    "            #print(\"@@@@\",word,list_testwords,list_tokens)\n",
    "            token_word = nlp(word)\n",
    "            istax, tot_similar_word, list_res_tokens=token_compare(token_word,list_tokens,min_score)\n",
    "            if istax:\n",
    "                for taxtoken in list_res_tokens:\n",
    "                    #print(\"@@@\",taxtoken)\n",
    "                    if taxtoken[1].lemma_.lower() not in list_keep_tax_words:\n",
    "                        list_keep_tax_words.append(taxtoken[1].lemma_.lower())\n",
    "                    \n",
    "                tot_similar_doc+=tot_similar_word\n",
    "            istax_doc |= istax\n",
    "        #print(\"@@#\",tot_similar_doc,similar_doc)\n",
    "        #print(\"$$\",n)\n",
    "        if istax_doc and (tot_similar_doc >= similar_doc):\n",
    "            list_keep_pointer_taxdocs.append(n)\n",
    "            #print(\"TOTAL for doc:\",n,\"  score:\",tot_similar_doc)\n",
    "            #print(txtdoc)\n",
    "    return list_keep_pointer_taxdocs , list_keep_tax_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_keep_tax_words=[]\n",
    "#step1\n",
    "list_keywords=['belasting','tax','fisc']\n",
    "list_keep_pointer_taxdocs , list_keep_tax_words = createlistofkeywords(10,2,0.95,list_keywords)\n",
    "print(\"step1:\",list_keywords)\n",
    "print(\"\\n------------------------------------------------------------------------------------\")\n",
    "#step2\n",
    "list_keywords+=list_keep_tax_words\n",
    "list_keep_pointer_taxdocs , list_keep_tax_words = createlistofkeywords(20,4,0.90,list_keywords)\n",
    "print(\"step2:\",list_keywords)\n",
    "print(\"\\n------------------------------------------------------------------------------------\")\n",
    "\n",
    "#step3\n",
    "list_keywords+=list_keep_tax_words\n",
    "list_keep_pointer_taxdocs , list_keep_tax_words = createlistofkeywords(40,8,0.85,list_keywords)\n",
    "print(\"step3:\",list_keywords)\n",
    "print(\"\\n------------------------------------------------------------------------------------\")\n",
    "\n",
    "#step4\n",
    "list_keywords+=list_keep_tax_words\n",
    "list_keep_pointer_taxdocs , list_keep_tax_words = createlistofkeywords(80,16,0.80,list_keywords)\n",
    "print(\"step4:\",list_keywords)\n",
    "print(\"\\n------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "#final\n",
    "list_keywords+=list_keep_tax_words\n",
    "print(list_keywords,\"\\n  taxdocus are: \",list_keep_pointer_taxdocs)\n",
    "print(\"step5:\",list_keywords)\n",
    "print(\"\\n------------------------------------------------------------------------------------\")\n",
    "\n",
    "df_keywords=pd.DataFrame(list_keywords,columns=['keywords'])\n",
    "#save complete list of keywords\n",
    "df_keywords.to_pickle(\"../data/tax_keywords.pkl\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_keywords = pd.read_pickle(\"../data/tax_keywords.pkl\") \n",
    "list_keywords=list(df_keywords['keywords'])\n",
    "\n",
    "#run on all documents\n",
    "\n",
    "#show texts that are categorized as tax document\n",
    "#for p in list_keep_pointer_taxdocs:#\n",
    "    #print(\"-------------------------------------------------------------------------------\")\n",
    "    #print( df['cleantextnl'].values[p] )\n",
    "    #input()\n",
    "#print(\"docu: \",n,\"######################################################################################################\")\n",
    "####################################################################\n",
    "# output: list_keep_pointer_taxdocs , list_keep_tax_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start for all\")\n",
    "list_keep_pointer_taxdocs , list_keep_tax_words = createlistofkeywords(10,0,0.95,list_keywords)\n",
    "print(\"\\n  taxdocus are: \",list_keep_pointer_taxdocs)\n",
    "print(\"end for all\")\n",
    "print(\"stepEND:\",list_keywords)\n",
    "print(\"\\n------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f76d795eeeb227e2159e85f1e474be6be927cb377d2f5f811780d197b385e423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
