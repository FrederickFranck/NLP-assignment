{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPMG analyse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!python -m spacy download nl_core_news_md\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"nl_core_news_md\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some debugging stuff\n",
    "# \n",
    "if DEBUG:\n",
    "    import inspect \n",
    "    def getname():\n",
    "        import sys\n",
    "        return sys._getframe(1).f_code.co_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_cleanandlemmatize(txtdoc: str, list_wordstoskip:str = ''):\n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "   \n",
    "    LANG='nl'\n",
    "    # words to discard\n",
    "    months={'nl':['januari','februari','maart','april','mei','juni','augustus','september','oktober','november','december'],\n",
    "            'fr':['janvier','fevrier','mars','avril','mai','juin','juillet','aout','septembre','octobre','decembre']}\n",
    "    days={'nl':['maandag','dinsdag','woensdag','donderdag','vrijdag','zaterdag','zondag'],\n",
    "            'fr':['lundi','mardi','mercredi','jeudi','vendredi','samedi','dimanche']}\n",
    "    # \n",
    "    nlp.max_length=10000000\n",
    "    nlp_doc=nlp(txtdoc)\n",
    "    list_allwordslemmatized=[]\n",
    "    dict_uniqwords={}\n",
    "    list_tokens=[]\n",
    "    #filter\n",
    "    for token in nlp_doc:\n",
    "        lemma_lower=token.lemma_.lower()\n",
    "        if token in stopwords:\n",
    "            continue\n",
    "        if (token.is_punct or token.is_space or token.is_stop):\n",
    "            continue\n",
    "        if token.text.isdecimal():\n",
    "            continue\n",
    "        if True in [char.isdigit() for char in token.text]:\n",
    "            continue\n",
    "        if token.text[-1] == '.':\n",
    "            continue\n",
    "        if len(token.text) <= 2:\n",
    "            continue\n",
    "        if lemma_lower in months[LANG]:\n",
    "            continue\n",
    "        if lemma_lower in days[LANG]:\n",
    "            continue\n",
    "        if lemma_lower in list_wordstoskip:\n",
    "            continue\n",
    "        #pass only nouns\n",
    "        if token.pos_ != 'NOUN':\n",
    "            continue\n",
    "        #create dict of unique words with count\n",
    "        if lemma_lower not in dict_uniqwords: \n",
    "            dict_uniqwords[lemma_lower]=1\n",
    "            #save tokens for vector comparison\n",
    "            list_tokens.append(token)\n",
    "        else:\n",
    "            dict_uniqwords[lemma_lower]+=1\n",
    "    return dict_uniqwords, list_tokens\n",
    "    #end of filter------------------------\n",
    "\n",
    "#compare a token against a tokenslist\n",
    "def token_compare(token_totest,list_tokens,min_score:int = 0.6):\n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "    \n",
    "    #return list of tokens with similarity >= min_score \n",
    "    list_tokens_toreturn=[]\n",
    "    tot_similar_word=0\n",
    "    istax=False\n",
    "    for token in list_tokens:\n",
    "        similar=token_totest.similarity(token)\n",
    "        #only addup the scores >= min_score\n",
    "        if similar >= min_score:\n",
    "            list_tokens_toreturn.append([token_totest,token,similar])\n",
    "            #positive for tax\n",
    "            istax=True\n",
    "            tot_similar_word+=similar\n",
    "    return istax, tot_similar_word, list_tokens_toreturn\n",
    "\n",
    "def createlistofkeywords(numberofdocumenttoscan:int = 50,similar_doc:int = 0,min_score:float = 0.6, list_keywords:list = []) -> (list,list,list):\n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "     \n",
    "    list_keep_tax_words=[]\n",
    "    list_keep_pointer_taxdocs=[]\n",
    "    list_keep_pointer_alldocs=[]\n",
    "\n",
    "    #take all docs if 0\n",
    "    if numberofdocumenttoscan == 0:\n",
    "        numberofdocumenttoscan=len(df)\n",
    "    \n",
    "    for n in range(numberofdocumenttoscan):\n",
    "\n",
    "        print(\"Documents analyzed: \",numberofdocumenttoscan,n)\n",
    "\n",
    "        txtdoc = df['cleantextnl'].values[n]\n",
    "        list_wordstoskip=['blabla','blablabla'] #add here words to discard\n",
    "        dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txtdoc,list_wordstoskip) #get tokens\n",
    "\n",
    "        #tokenize and check\n",
    "        tot_similar_doc=0     #keep score for onlytax docs (istax=True)\n",
    "        tot_similar_doc_all=0 #keep score for all docs\n",
    "        istax_doc=False\n",
    "        \n",
    "        for word in list_keywords:\n",
    "            token_word = nlp(word)\n",
    "            istax, tot_similar_word, list_res_tokens=token_compare(token_word,list_tokens,min_score)\n",
    "            #if DEBUG: print(\"DEBUG\",istax, tot_similar_word, list_res_tokens)\n",
    "            tot_similar_doc_all+=tot_similar_word\n",
    "\n",
    "            if istax:\n",
    "                for taxtoken in list_res_tokens:\n",
    "                    if taxtoken[1].lemma_.lower() not in list_keep_tax_words:\n",
    "                        list_keep_tax_words.append(taxtoken[1].lemma_.lower())\n",
    "                tot_similar_doc+=tot_similar_word\n",
    "            istax_doc |= istax\n",
    "\n",
    "        #store score for all\n",
    "        list_keep_pointer_alldocs.append([n,tot_similar_doc_all])\n",
    "\n",
    "        #store score for taxdocs\n",
    "        if istax_doc and (tot_similar_doc >= similar_doc):\n",
    "            list_keep_pointer_taxdocs.append([n,tot_similar_doc])\n",
    "\n",
    "    return list_keep_pointer_taxdocs, list_keep_tax_words, list_keep_pointer_alldocs\n",
    "\n",
    "def create_pickle_keywords_and_docscores(list_keywords:list = ['belasting'], file_keywords:str = \"\", file_docscores:str = \"\")-> (list,list):\n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "     \n",
    "    #settings : [numberofdocumenttoscan (0 for all), min similarity score for doc to get into the taxlist, min similarity score for keywords]):\n",
    "    settings=[  [5,5,0.97],    #step1\n",
    "                [10,10,0.95],  #step2\n",
    "                [15,20,0.90],  #step3\n",
    "                [0,40,0.87]    #step4 All documents\n",
    "                ]\n",
    "    \n",
    "    numberofsteps=len(settings)\n",
    "    for step in range(numberofsteps):\n",
    "        list_keep_pointer_taxdocs , list_keep_tax_words, list_keep_pointer_alldocs = createlistofkeywords(settings[step][0],settings[step][1],settings[step][2],list_keywords)\n",
    "        list_keywords+=list_keep_tax_words\n",
    "        #no duplicates\n",
    "        list_keywords = list(set(list_keywords))\n",
    "        list_docscores=list_keep_pointer_alldocs\n",
    "        \n",
    "        # no file no pickle\n",
    "        if file_keywords != \"\" :\n",
    "            df_keywords=pd.DataFrame(list_keywords,columns=['keywords'])\n",
    "            df_keywords.to_pickle(file_keywords)  \n",
    "            \n",
    "        if file_docscores != \"\"  :\n",
    "            df_docscores=pd.DataFrame(list_keep_pointer_alldocs,columns=['docpointer','docscores']) \n",
    "            df_docscores.to_pickle(file_docscores)  \n",
    "\n",
    "    return list_keywords, list_docscores\n",
    "\n",
    "def score_text(txt:str,language:str = 'nl', min_score:float = 0.3, file_keywords:str = \"../data/tax_keywords_nl.pkl\") -> float :\n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "\n",
    "    if language == 'nl':\n",
    "        df_keywords = pd.read_pickle(file_keywords) \n",
    "        list_keywords=list(df_keywords['keywords'])\n",
    "        \n",
    "        #clean txt / get tokens\n",
    "        dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txt,[]) \n",
    "        \n",
    "        docscore=0  \n",
    "        for word in list_keywords:\n",
    "            token_word = nlp(word)\n",
    "            for token in list_tokens:\n",
    "                similar=token_word.similarity(token)\n",
    "                #similarity > min_score to be taken into account\n",
    "                if similar >= min_score:\n",
    "                    docscore+=similar\n",
    "    else:\n",
    "        print(\"Language selection not supported for now!\")\n",
    "\n",
    "    return docscore\n",
    "\n",
    "def create_initial_keywordlist(language:str ='nl') -> None:\n",
    "    # create keyword list picklefile and docscores picklefile\n",
    "    #read all docs\n",
    "    global df\n",
    "    df = pd.read_pickle(\"../data/Staatsblad_nl_fr.pkl\") \n",
    "    if language == 'nl':\n",
    "        #start search for nl keywords\n",
    "        keywords, docscores = create_pickle_keywords_and_docscores(['belasting','tax','fisc'], \"../data/tax_keywords_nl.pkl\", \"../data/tax_docscores_nl.pkl\")\n",
    "    else:\n",
    "        print(\"Language selection not supported for now!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will create the inital keyword list (takes some time to run +-7min on my old laptop) and needs to be run only once!\n",
    "#df = pd.read_pickle(\"../data/Staatsblad_nl_fr.pkl\") \n",
    "create_initial_keywordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bmadmin\\AppData\\Local\\Temp\\ipykernel_15348\\2622093757.py:157: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similar=token_word.similarity(token)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for d: 32.10280970434261 score for d2: 0\n"
     ]
    }
   ],
   "source": [
    "#Get score for one txt string\n",
    "#############################\n",
    "#This will use the pickled keyword list created by the create_initial_keywordlist() function\n",
    "txt='de belastingen zijn er weer\\n Deze tax keer meer belastingen en meer tax te betalen!\\n meer en meer belastingen tax is nodig en fisc'\n",
    "txt2='this text does not contain any ... related words\\n '\n",
    "d = score_text(txt)\n",
    "d2 = score_text(txt2)\n",
    "print(\"score for d:\",d,\"score for d2:\",d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS THE END FOR NOW ..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f76d795eeeb227e2159e85f1e474be6be927cb377d2f5f811780d197b385e423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
