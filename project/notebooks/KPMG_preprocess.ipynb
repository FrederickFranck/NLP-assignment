{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPMG TEST NOTEBOOK\n",
    "\n",
    "Selenium is a tool initially created to automate tests on websites. It is therefore very useful when information is accessible by clicking on links. A button for example is an element from which it is very difficult to obtain the link. BeautifulSoup then becomes limited.\n",
    "In this case, use Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "BRUSSELS HOOFDSTEDELIJK GEWEST  \n",
      "---  \n",
      "  \n",
      "### _8 JANUARI 2020. - Ministerieel besluit tot vaststelling van de modellen\n",
      "van formulier bedoeld in artikel 8, § 1 van het besluit van de Brusselse\n",
      "Hoofdstedelijke Regering van 19 januari 2017 tot vaststelling van de\n",
      "modaliteiten van het gunstregime toepasselijk op de successierechten bij\n",
      "overdracht van familiale ondernemingen en familiale vennootschappen _\n",
      "\n",
      "  \n",
      "  \n",
      "De Minister van de Brusselse Hoofdstedelijke Regering, belast met Financiën en\n",
      "Begroting,  \n",
      "Gelet op het Wetboek der Successierechten, artikel 60bis/2, § 1, ingevoegd\n",
      "door de ordonnantie van 12 december 2016 houdende het tweede deel van de\n",
      "fiscale hervorming;  \n",
      "Gelet op het besluit van de Brusselse Hoofdstedelijke Regering van 19 januari\n",
      "2017 tot vaststelling van de modaliteiten van het gunstregime toepasselijk op\n",
      "de successierechten bij overdracht van familiale ondernemingen en familiale\n",
      "vennootschappen, artikel 8;  \n",
      "Gelet op de gelijkekansentest uitgevoerd in toepassing van artikel 2 van de\n",
      "ordonnantie van 4 oktober 2018 tot invoering van de gelijkekansentest;  \n",
      "Overwegende dat artikel 8 van bovengenoemd besluit stelt dat de Minister van\n",
      "de Brusselse Hoofdstedelijke Regering, belast met Financiën en Begroting,\n",
      "bevoegd is om het formulier dat bedoeld is om na te gaan of de voorwaarden\n",
      "voor het behoud van de het fiscale gunstregime, dat werd bekomen in het kader\n",
      "van de overdracht van een familiale onderneming of een familiale vennootschap,\n",
      "vervuld zijn gebleven tot het einde van de driejarige periode die aanving bij\n",
      "het overlijden van de decujus, zoals bedoeld in artikel 60bis/2, § 1, van het\n",
      "Wetboek der Successierechten;  \n",
      "Overwegende dat dit ministerieel besluit geen reglementaire voorschriften\n",
      "bevat in de zin van artikel 3, § 1, eerste lid, van de gecoördineerde wetten\n",
      "van de Raad van State van 12 januari 1973, is het niet onderworpen aan het\n",
      "advies van de Raad van State, afdeling Wetgeving,  \n",
      "Besluit :  \n",
      "Artikel 1. Het formulier dat bedoeld is om na te gaan of de voorwaarden voor\n",
      "het behoud van de het fiscale gunstregime, dat werd bekomen in het kader van\n",
      "de overdracht van een familiale onderneming, vervuld zijn gebleven tot het\n",
      "einde van de driejarige periode die aanving bij het overlijden van de decujus,\n",
      "zoals bedoeld in artikel 60bis/2, § 1 van het Wetboek der Successierechten, is\n",
      "hernomen in bijlage 1 van dit besluit.  \n",
      "Art. 2. Het formulier dat bedoeld is om na te gaan of de voorwaarden voor het\n",
      "behoud van het fiscale gunstregime, dat werd bekomen in het kader van het\n",
      "overdracht van een familiale vennootschap, vervuld zijn gebleven tot het einde\n",
      "van de driejarige periode die aanving bij het overlijden van de decujus, zoals\n",
      "bedoeld in artikel 60bis/2, § 1 van het Wetboek der Successierechten, is\n",
      "hernomen in bijlage 2 bij dit besluit.  \n",
      "Art. 3. Dit besluit heeft uitwerking met ingang van 1 januari 2020.  \n",
      "Brussel, 8 januari 2020.  \n",
      "De Minister van de Brusselse Hoofdstedelijke Regering, belast met Financiën,\n",
      "Begroting, Openbaar ambt en de Promotie van Meertaligheid,  \n",
      "S. GATZ  \n",
      "  \n",
      "Voor de raadpleging van de tabel, zie beeld  \n",
      "  \n",
      "  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_pickle(\"../data/Staatsblad_nl_fr.pkl\") \n",
    "print(df['cleantextnl'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15:\n",
      "#######\n",
      "---------------------------------------------------\n",
      "[('besluit', 8), ('successierecht', 6), ('regering', 5), ('gunstregime', 5), ('overdracht', 5), ('formulier', 4), ('onderneming', 4), ('vennootschap', 4), ('wetboek', 4), ('vaststelling', 3)]\n",
      "---------------------------------------------------\n",
      "[('besluit', 4), ('tarief', 4), ('vertaling', 4), ('januar', 4), ('minister', 4), ('dienst', 3), ('sätzen', 3), ('wörter', 3), ('wijziging', 2), ('vaststelling', 2)]\n",
      "---------------------------------------------------\n",
      "[('gutes', 17), ('nicht', 9), ('auf', 9), ('ke/estgb', 7), ('kostenlose', 6), ('zurverfügungstellung', 6), ('vorteil', 6), ('minister', 5), ('goed', 4), ('vertaling', 4)]\n",
      "---------------------------------------------------\n",
      "[('onderwijs', 14), ('promotie', 13), ('afdeling', 9), ('wetenschap', 7), ('specialisatie', 6), ('besluit', 6), ('code', 5), ('type', 4), ('gebied', 3), ('decreet', 3)]\n",
      "---------------------------------------------------\n",
      "[('besluit', 9), ('minister', 6), ('overwegende', 4), ('aftrek', 4), ('financiën', 4), ('wetboek', 3), ('inkomstenbelasting', 3), ('groepsbijdrage', 3), ('wet', 2), ('kb/wib', 2)]\n",
      "---------------------------------------------------\n",
      "[('besluit', 15), ('wetboek', 6), ('inkomstenbelasting', 6), ('uitvoering', 5), ('aanduiding', 4), ('toepassing', 4), ('ministerieel', 2), ('minister', 2), ('wet', 2), ('overwegende', 2)]\n",
      "---------------------------------------------------\n",
      "[('vennootschap', 83), ('aandeel', 40), ('jaar', 38), ('recht', 32), ('voorwaarde', 29), ('belas_tingvermindering', 26), ('deelneming', 22), ('inschrijver', 22), ('maand', 21), ('starters_fonds', 20)]\n",
      "---------------------------------------------------\n",
      "[('onderwijs', 13), ('promotie', 12), ('afdeling', 9), ('specialisatie', 6), ('accountant', 6), ('code', 5), ('besluit', 5), ('type', 4), ('gebied', 3), ('decreet', 3)]\n",
      "---------------------------------------------------\n",
      "[('raadpleging', 19), ('tabel', 19), ('beeld', 19), ('besluit', 2), ('staatsblad', 2), ('editie', 2), ('akte', 2), ('woord', 2), ('rekening', 2), ('minister', 2)]\n",
      "---------------------------------------------------\n",
      "[('raadpleging', 19), ('tabel', 19), ('beeld', 19), ('besluit', 2), ('staatsblad', 2), ('editie', 2), ('akte', 2), ('woord', 2), ('rekening', 2), ('minister', 2)]\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_news_md\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"nl_core_news_md\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "def dict_sort(dict_uniqwords:dict ) -> dict:\n",
    "    from operator import itemgetter\n",
    "    srt=sorted(dict_uniqwords.items(), key=itemgetter(1),reverse=True)\n",
    "    return srt\n",
    "\n",
    "def getsomestats(dict_uniqwords:dict ,txt_original:str, topcnt:int = 15):\n",
    "    #---NOT USED FOR NOW\n",
    "\n",
    "    #print(\"@@@@@@@@@@@@@@@@@@@@\",dict_uniqwords)\n",
    "    # total number of unique words in doc\n",
    "    print(\"unique word count :\", len(dict_uniqwords))\n",
    "\n",
    "    # total number of words in doc\n",
    "    wordcnt=0\n",
    "    for s in dict_uniqwords: wordcnt+=dict_uniqwords[s]\n",
    "    print(\"total wordcount: \",wordcnt)\n",
    "    \n",
    "    #sorted words count\n",
    "    ###from operator import itemgetter\n",
    "    ###srt=sorted(dict_uniqwords.items(), key=itemgetter(1),reverse=True)\n",
    "    srt = dict_sort(dict_uniqwords)\n",
    "\n",
    "    res=[]\n",
    "    for i in range(topcnt):\n",
    "        try:\n",
    "            res.append(srt[i][0])\n",
    "            print(srt[i])\n",
    "        except:\n",
    "            break\n",
    "    #print original doc\n",
    "    #print(txt_original)\n",
    "    return res\n",
    "\n",
    "def nlp_cleanandlemmatize(txtdoc: str, list_wordstoskip:str = ''):\n",
    "    LANG='nl'\n",
    "    # Load the model\n",
    "    #nlp = spacy.load(\"nl_core_news_md\")\n",
    "    #stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "    # words to discard\n",
    "    months={'nl':['januari','februari','maart','april','mei','juni','augustus','september','oktober','november','december'],\n",
    "            'fr':['janvier','fevrier','mars','avril','mai','juin','juillet','aout','septembre','octobre','decembre']}\n",
    "    days={'nl':['maandag','dinsdag','woensdag','donderdag','vrijdag','zaterdag','zondag'],\n",
    "            'fr':['lundi','mardi','mercredi','jeudi','vendredi','samedi','dimanche']}\n",
    "    # \n",
    "    nlp.max_length=10000000\n",
    "    nlp_doc=nlp(txtdoc)\n",
    "    list_allwordslemmatized=[]\n",
    "    dict_uniqwords={}\n",
    "    list_tokens=[]\n",
    "    #sequential filter\n",
    "    for token in nlp_doc:\n",
    "        lemma_lower=token.lemma_.lower()\n",
    "        if token in stopwords:\n",
    "            continue\n",
    "        if (token.is_punct or token.is_space or token.is_stop):\n",
    "            continue\n",
    "        if token.text.isdecimal():\n",
    "            continue\n",
    "        if True in [char.isdigit() for char in token.text]:\n",
    "            continue\n",
    "        if token.text[-1] == '.':\n",
    "            continue\n",
    "        if len(token.text) <= 2:\n",
    "            continue\n",
    "        if lemma_lower in months[LANG]:\n",
    "            continue\n",
    "        if lemma_lower in days[LANG]:\n",
    "            continue\n",
    "        if lemma_lower in list_wordstoskip:\n",
    "            continue\n",
    "        #pass only nouns\n",
    "        #print(token.pos_,token.pos)\n",
    "        if token.pos_ != 'NOUN':\n",
    "            continue\n",
    "        #create dict of unique words with count\n",
    "        if lemma_lower not in dict_uniqwords: \n",
    "            dict_uniqwords[lemma_lower]=1\n",
    "            #save tokens for vector comparison\n",
    "            list_tokens.append(token)\n",
    "        else:\n",
    "            dict_uniqwords[lemma_lower]+=1\n",
    "        ##save tokens for vector comparison\n",
    "        ##list_tokens.append(token)\n",
    "\n",
    "        \n",
    "        #saving the words (lemma equates number   -   lemma_ equates word)\n",
    "        ###list_allwordslemmatized.append([token.lemma,lemma_lower,token.text])\n",
    "    return dict_uniqwords, list_tokens\n",
    "    #end of filter-----------------------------------------------\n",
    "\n",
    "def concat_docs(docs:list[str] ,nmbr:int = 0) -> str:\n",
    "    #concat nmbr dcocuments from doc return txtdoc\n",
    "    #nmbr=10 #test anz docus\n",
    "    txtdoc=''\n",
    "    #for txt in df['cleantextnl'].values:\n",
    "    for txt in docs:\n",
    "        txtdoc+=str(txt)\n",
    "        if nmbr == 1:\n",
    "            break\n",
    "        nmbr-=1\n",
    "    return txtdoc\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"../data/Staatsblad_nl_fr.pkl\") \n",
    "\n",
    "print(\"Top 15:\\n#######\")\n",
    "numberofdocumenttoscan=10\n",
    "for n in range(numberofdocumenttoscan):\n",
    "    #how many docs to use for testing\n",
    "    #txtdoc = concat_docs(df['cleantextnl'].values, 10)\n",
    "    txtdoc = df['cleantextnl'].values[n]\n",
    "\n",
    "    list_wordstoskip=['artikel','document'] #add here words to discard\n",
    "\n",
    "    dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txtdoc,list_wordstoskip)\n",
    "\n",
    "    list_sorted_uniqwords = dict_sort(dict_uniqwords)\n",
    "\n",
    "    ###print(\"Remaining words after filtering: \",len(list_allwordslemmatized))\n",
    "    #topcnt=10 \n",
    "    #list_word_topcnt=getsomestats(dict_uniqwords,txtdoc,topcnt)\n",
    "    #print(\"Top\",topcnt,\": \",list_word_topcnt)\n",
    "    ##################################################################\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    ##################################################################\n",
    "    # Output:   list_sorted_uniqwords , dict_uniqwords, list_tokens\n",
    "    #\n",
    "    print(list_sorted_uniqwords[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --------------------------\n",
      "[belasting, modaliteiten, 0.6629545770614875] \n",
      "\n",
      "[belasting, successierechten, 0.9043948372239012] \n",
      "\n",
      "[belasting, modaliteiten, 0.6629545770614875] \n",
      "\n",
      "1 --------------------------\n",
      "[belasting, belasting, 1.0] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bmadmin\\AppData\\Local\\Temp\\ipykernel_10112\\2716612306.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similar=token_totest.similarity(token)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 --------------------------\n",
      "3 --------------------------\n",
      "4 --------------------------\n",
      "[belasting, inkomstenbelastingen, 0.8263346384300361] \n",
      "\n",
      "[belasting, interesten, 0.7032102702518628] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compare a token against a tokenslist\n",
    "def token_compare(token_totest,list_tokens,min_score:int = 60):\n",
    "    list_tokens_toreturn=[]\n",
    "    for token in list_tokens:\n",
    "        #print(\"token_compare\",token)\n",
    "        similar=token_totest.similarity(token)\n",
    "        #print(token_totest, \"<->\", token, similar)\n",
    "        if similar >= min_score/100:\n",
    "            list_tokens_toreturn.append([token_totest,token,similar])\n",
    "    return list_tokens_toreturn\n",
    "\n",
    "numberofdocumenttoscan=5\n",
    "for n in range(numberofdocumenttoscan):\n",
    "    txtdoc = df['cleantextnl'].values[n]\n",
    "    list_wordstoskip=['artikel','document'] #add here words to discard\n",
    "    dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txtdoc,list_wordstoskip) #get tokens\n",
    "    #list_sorted_uniqwords = dict_sort(dict_uniqwords) #sort\n",
    "    \n",
    "    print(n,\"--------------------------\")\n",
    "    test=nlp('belasting')\n",
    "    res=token_compare(test,list_tokens,65)\n",
    "    for i in res:\n",
    "        print(i,'\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all2['cleantextnl'][728])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "\n",
    "\n",
    "#df['cleantextnl']=''\n",
    "#df['cleantextfr']=''\n",
    "pnt=0\n",
    "for language in ['fr','nl']:\n",
    "    \n",
    "    for url in df['Link NL']:\n",
    "        htmllanguage='language='+language\n",
    "        url=url.replace('language=nl',htmllanguage)\n",
    "        url2='view-source:'+url\n",
    "        url2=url2.replace('.pl?','_body.pl?')\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(url2)\n",
    "        #simulate human \n",
    "        rnd=randrange(1,2)\n",
    "        sleep(rnd)\n",
    "        page=driver.page_source\n",
    "        driver.close()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html\").text\n",
    "\n",
    "        dummy=html2text.html2text(soup)\n",
    "        dummy=dummy.split('---|---|---|---|---|---')\n",
    "        \n",
    "        dummy=dummy[1].split('begin |  |  eerste woord |  laatste woord |  |')\n",
    "        cleantext=dummy[0]\n",
    "        #print(cleantext)\n",
    "        #save \n",
    "        txtlabel='cleantext'+language\n",
    "        df[txtlabel][pnt]=cleantext\n",
    "        pnt+=1\n",
    "        #if pnt == 1:\n",
    "        #    pnt=0\n",
    "        #    break\n",
    "        \n",
    "\n",
    "df.to_pickle(\"../data/Staatsblad.pkl\")  \n",
    "unpickled_df = pd.read_pickle(\"../data/Staatsblad.pkl\") \n",
    "unpickled_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install html2text\n",
    "import html2text\n",
    "\n",
    "a=html2text.html2text(soup)\n",
    "a=a.split('---|---|---|---|---|---')\n",
    "a=a[1].split('begin |  |  eerste woord |  laatste woord |  |')\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "b=a\n",
    "\n",
    "b=b.split('\\n')\n",
    "\n",
    " \n",
    "for i in b:\n",
    "   \n",
    "    print(remove_html_tags(i),'\\n')\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "\n",
    "If you are missing any libraries in the next cell, you'll need to install them before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import lxml.html\n",
    "import time\n",
    "import random\n",
    "from random import randint\n",
    "import logging\n",
    "import collections\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "\n",
    "date = strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Selenium according to this manual\n",
    "\n",
    "https://selenium-python.readthedocs.io/installation.html#downloading-python-bindings-for-selenium/bin\n",
    "\n",
    "*NB: On Linux, put your `geckodriver` (the downloaded extension) in the equivalent path on your machine into `/home/<YOUR_NAME>/.local/bin/`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simulate a search on the official Python website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "# The selenium.webdriver module provides all the implementations of WebDriver\n",
    "# Currently supported are Firefox, Chrome, IE and Remote. The `Keys` class provides keys on\n",
    "# the keyboard such as RETURN, F1, ALT etc.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Here, we create instance of Firefox WebDriver.\n",
    "#driver = webdriver.Firefox()\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# The driver.get method will lead to a page given by the URL. WebDriver will wait until the page is fully\n",
    "# loaded (i.e. the \"onload\" event has been triggered) before returning the control to your script.\n",
    "# It should be noted that if your page uses a lot of AJAX calls when loading, WebDriver may not know\n",
    "# when it was fully loaded.\n",
    "driver.get(\"http://www.python.org\")\n",
    "\n",
    "# The following line is a statement confirming that the title contains the word \"Python\".\n",
    "assert \"Python\" in driver.title\n",
    "\n",
    "# WebDriver offers several methods to search for items using one of the methods\n",
    "# `find_element_by_...` .\n",
    "# For example, the input text element can be located by its name attribute by\n",
    "# using the `find_element_by_name` method.\n",
    "elem = driver.find_element_by_name(\"q\")\n",
    "\n",
    "# Then we send keys. This is similar to entering keys using your keyboard.\n",
    "# Special keys can be sent using the `Keys` class imported in line 7 (from selenium.webdriver.common.keys import Keys).\n",
    "# For security reasons, we will delete any pre-filled text in the input field\n",
    "# (for example, \"Search\") so that it does not affect our search results:\n",
    "elem.clear()\n",
    "elem.send_keys(\"pycon\")\n",
    "elem.send_keys(Keys.RETURN)\n",
    "\n",
    "# After submitting the page, you should get the result if there is one. To ensure that certain results\n",
    "# are found, make an assertion:\n",
    "assert \"No results found.\" not in driver.page_source\n",
    "try:\n",
    "    assert \"Trademark Usage Policy\" in driver.page_source\n",
    "except:\n",
    "    print(\"ok I found this text \\\"Trademark Usage Policy\\\"\")\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the source code of the webpage and check that the search area (field) is called \"q\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"http://www.python.org\")\n",
    "assert \"Python\" in driver.title\n",
    "try: \n",
    "    elem = driver.find_element_by_name(\"q\")\n",
    "    print(\"element q has been found!\")\n",
    "except:\n",
    "    print(\"element q has NOT been found!\")\n",
    "    driver.close()\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a phone number from *leboncoin*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#url = \"https://www.leboncoin.fr/sports_hobbies/1536839557.htm/\"\n",
    "url = \"https://www.leboncoin.fr/sports_hobbies/2112572252.htm\"\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(url)\n",
    "\n",
    "python_button = driver.find_elements_by_xpath('//div[@data-reactid=\"269\"]')[0]\n",
    "python_button.click()\n",
    "input(\"Waiting...\")\n",
    "# And then we use Beautiful soup\n",
    "soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "for elem in soup.find_all(\"a\", attrs={\"data-qa-id\": \"adview_number_phone_contact\"}):\n",
    "    print(elem.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting from *leboncoin*, collect all the information available to define the product being sold. Use `selenium` for the telephone number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API (Application Program Interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of tools and methods that allow different applications to interact with each other. In the case of a web service, we can retrieve data dynamically. By using an API correctly, we can thus obtain in real time, the modifications made on a \"parent\" site.\n",
    "\n",
    "For example, we will retrieve online news, for example from the \"L'équipe\" website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions at https://newsapi.org/s/lequipe-api to retrieve an \"API key\" connection key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your API key is: `73bbb95f8ecb49b499113a46481b4af1`\n",
    "\n",
    "\n",
    "It is frequent that a key does not work after a while (e.g. `5 min`n `30 min`, a day, ...)\n",
    "So don't jump up if you get an error message back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "key = \"73bbb95f8ecb49b499113a46481b4af1\"\n",
    "url = \"https://newsapi.org/v2/top-headlines?sources=lequipe&apiKey=\" + key\n",
    "response = requests.get(url)\n",
    "\n",
    "# Here the response format is a json file, it is used as a dictionary\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionnary = response.json()\n",
    "print(dictionnary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in list(dictionnary.keys()):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Key: \", element, \"// Values: \", dictionnary[element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we have lists in dictionaries(it's a JSON file actually but it's very similar)\n",
    "# We will discover the information of the article key.\n",
    "\n",
    "for element in enumerate(dictionnary[\"articles\"]):\n",
    "    print(\"###############################################\")\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So if we keep going, it gives us another dictionary!\n",
    "for element in dictionnary[\"articles\"][0].keys():\n",
    "    print(\" Key : \", element, \"Values : \", dictionnary[\"articles\"][0][element])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a script that allows you to take details of the last ten news from the team or another site. Store them in a nice CSV or excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "newslist=[]\n",
    "for element in dictionnary[\"articles\"]:\n",
    "    #print(element.keys())\n",
    "    \n",
    "    newslist.append([element['publishedAt'],element['content']])\n",
    "    newslist.sort(reverse=True)\n",
    "\n",
    "#for i in newslist:\n",
    "#    print(i[0],\" ***************** \",i[1])\n",
    "    \n",
    "print(type(newslist),len(newslist))\n",
    "publishedat=[]\n",
    "content=[]\n",
    "\n",
    "for i in newslist:\n",
    "    publishedat.append(i[0])\n",
    "    content.append(i[1])\n",
    "#print(len(publishedat),len(content))\n",
    "#print(publishedat)\n",
    "#print(content)\n",
    "#break\n",
    "df = pd.DataFrame({\"publishedAt\": publishedat})\n",
    "df[\"Content\"] = content   \n",
    "#print(df)\n",
    "\n",
    "df.to_csv(\"./assets/orderedNewsList.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f76d795eeeb227e2159e85f1e474be6be927cb377d2f5f811780d197b385e423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
